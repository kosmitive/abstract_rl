import numpy as np
import torch
from torch.autograd import Variable
from torch.distributions import Normal, Beta
from torch.nn import Parameter

from abstract_rl.src.misc.cli_printer import CliPrinter
from abstract_rl.src.neural_modules.neural_network import MultilayerPerceptron
from abstract_rl.src.data_structures.abstract_conf.model_configuration import ModelConfiguration
from abstract_rl.src.policy.continuous.continuous_policy import ContinuousPolicy


class UnivariateBetaPolicy(ContinuousPolicy):
    """
    Represents a univariate gaussian policy. This only supports one action dimension.
    """

    def __init__(self, mc):
        """
        Initializes a new uni variate gaussian policy.
        :param mc: The model configuration to use.
        """
        super().__init__(mc)
        assert isinstance(mc, ModelConfiguration)

        self.mc = mc
        run_conf = mc.get('conf')
        self.conf = run_conf
        self.h = mc['env'].action_space.high[0]

        # create policy network
        sd = mc.env.observation_dim
        policy_net_structure = np.concatenate(
            (
                [sd],
                run_conf['structure'],
                [2]
            ))

        af = run_conf['act_fn']
        self.network = MultilayerPerceptron(
            mc,
            hidden_neurons=policy_net_structure,
            act_fn=af,
            layer_norm=run_conf['layer_norm'],
            init=run_conf['init']
        )
        self.add_module('nn', self.network)

    def get_suff_stats_dim(self):
        """
        Obtain the dimension of sufficient statistics vector for a univariate gaussian.
        :return: depending on the configuration 1 for mean-only and 2 for mean and covariance.
        """

        return 2

    def mode(self, states=None, suff_stats=None):

        # check if it full fills a xor relation
        assert states is not None or suff_stats is not None
        assert states is None or suff_stats is None

        # get suff statistics
        if suff_stats is None: suff_stats = self.forward(states)
        alpha = suff_stats[:, 0:1]
        beta = suff_stats[:, 1:2]
        return (alpha - 1) / (alpha + beta - 2)

    def forward(self, states):
        """
        Simply produces sufficient statistics of the univariate gaussian policy.
        :param states: A matrix of shape [D, Ds] where each row corresponds to one state.
        :return: A matrix of shape [D, 2]
        """

        return self.network.forward(states) + 1

    def log_prob(self, actions, states=None, suff_stats=None):
        """
        Can be used to obtain log probability of actions relative to states.
        :param actions: A matrix of size [D, K] containing actions for each state.
        :param states: A matrix of size [D, Ds]. If this None => suff_states not None
        :param suff_stats: A matrix of size [D, get_suff_stats_dim()] containing suff stats.
            If this None => states is not None
        :return: A matrix [D, K] containing the log prob for each action in each row of
            actions matched to the row and the the suff stats of the specific state.
        """

        # check if it full fills a xor relation
        assert states is not None or suff_stats is not None
        assert states is None or suff_stats is None

        # get suff statistics
        if suff_stats is None: suff_stats = self.forward(states)

        # extract mu and sigma depending on settings
        alpha, beta = torch.split(suff_stats, 1, dim=1)
        dist = torch.distributions.Beta(alpha, beta)
        ll = dist.log_prob((actions - self.h) / (2 * self.h))
        return ll

    def sample_actions(self, states=None, suff_stats=None, num_actions=1):
        """
        Can be used to obtain action samples for all states.
        :param states: A matrix of size [D, Ds]. If this None => suff_states not None
        :param suff_stats: A matrix of size [D, get_suff_stats_dim()] containing suff stats.
            If this None => states is not None
        :param num_actions: The number of actions or K.
        :return: A matrix [D, K] containing action samples generated by the suff stats of the specific state.
        """

        # check if it full fills a xor relation
        assert states is not None or suff_stats is not None
        assert states is None or suff_stats is None

        # get suff statistics
        if suff_stats is None: suff_stats = self.forward(states)

        # extract mu and sigma depending on settings
        mu, sigma = torch.split(suff_stats, 1, dim=1)
        std_normal = Beta(mu[:, 0], sigma[:, 0])
        samples = std_normal.rsample([num_actions])
        return samples.t().contiguous() * (2 * self.h) + self.h

    def entropy(self, batch):
        t_states = torch.Tensor(batch.states)
        suff_stats = self.forward(t_states)

        mu, sigma = torch.split(suff_stats, 1, dim=1)
        std_normal = Beta(mu[:, 0], sigma[:, 0])
        ent = std_normal.entropy().mean()
        return ent

    def kl_divergence(self, batch, suff_stats=None, mode='i-projection', est='sum'):
        """
        Calculates the kl divergence between the current policy and the saved likelihood values.
        :param batch:
        :return:
        """

        # calc sufficient statistics
        states = torch.Tensor(batch.states)
        if suff_stats is None: suff_stats = self.forward(states)

        # create this distribution
        mu, sigma = torch.split(suff_stats, 1, dim=1)
        std_normal = Beta(mu[:, 0], sigma[:, 0])

        # create this distribution
        # get the old sufficient statistics
        with torch.no_grad():
            t_policy = self.mc.get('policy', True)
            t_suff_stats = t_policy.forward(states)
            t_mu, t_sigma = torch.split(t_suff_stats, 1, dim=1)
            t_std_normal = Beta(t_mu[:, 0], t_sigma[:, 0])

        # use I projection
        if mode == 'i-projection':
            p = std_normal
            q = t_std_normal

        # use M Projection
        elif mode == 'm-projection':
            q = std_normal
            p = t_std_normal

        else: raise NotImplementedError

        kl = torch.distributions.kl.kl_divergence(p, q)

        if est == 'sum': return kl.sum()
        elif est == 'mean': return kl.mean()
        else: raise NotImplementedError

    def count_parameters(self):
        """https://wikimedia.org/api/rest_v1/media/math/render/svg/50106a787db7d72ce3066a5a3238813cffebcc2e
        Number of parameters.
        :return: Gives a count of all parameters.
        """
        net_params = self.network.num_params()
        return net_params

    def get_gradients(self):
        """
        Gets gradients loaded internally.
        :return: A vector containing the gradients.
        """
        g = self.network.get_gradients()
        if self.cov_type == 'single': g = torch.cat([g, self.cov_variable.grad.data])
        return g

    def reset_gradients(self):
        """
        Resets gradients to zero.
        """

        # zero grads
        for p in self.network.parameters():
            p.grad.data.detach_()
            p.grad.data.zero_()

    def flip_gradients(self):
        """
        Set gradient itself from the outside.
        :param vector the gradient vector to set.
        """
        self.network.flip_gradients()

    def set_gradients(self, vec):
        """
        Set gradient itself from the outside.
        :param vector the gradient vector to set.
        """
        self.network.set_gradients(vec)

    def apply_gradients(self, lr):
        self.network.apply_gradients(lr)

    def clone(self):
        """
        Clones the current instance.
        :return: A new identical clones instance of the current.
        """

        # first of all clone the module using the subclass and
        # then fill all hierarchically lower elements.
        v = UnivariateBetaPolicy(self.mc)
        self.sync(v)
        return v

    def sync(self, v):
        """
        Copies the content of the current instance to the supplied parameter instance.
        :param v: The other instance, where the current values should be copied to.
        """
        self.network.sync(v.network)
        return v

    def print_block(self):
        cli = CliPrinter().instance
        cli.line().print(f"gaussian policy")
        w = self.network.get_params()
        norm_w = torch.dot(w, w).detach().numpy()
        cli.print(f"|w| \t=\t{norm_w}")
